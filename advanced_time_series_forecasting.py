# -*- coding: utf-8 -*-
"""advanced_time_series_forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pgE2o-tTr3161T3CTcZftaSZGXW8v1hh
"""

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import mean_squared_error, mean_absolute_error

# ===============================
# Configuration
# ===============================
SEQ_LEN = 30
HORIZON = 1
EPOCHS = 20
BATCH_SIZE = 32
LR = 0.001
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ===============================
# Synthetic Multivariate Time Series
# (5 features, 1200 time steps)
# ===============================
np.random.seed(42)

time = np.arange(1200)

data = np.stack([
    np.sin(0.02 * time) + 0.1 * np.random.randn(1200),
    np.cos(0.015 * time) + 0.1 * np.random.randn(1200),
    0.5 * np.sin(0.05 * time),
    0.3 * time / 1200,
    np.random.randn(1200) * 0.05
], axis=1)

# Normalize features
data = (data - data.mean(axis=0)) / data.std(axis=0)

# ===============================
# Sequence creation
# ===============================
def create_sequences(data, seq_len, horizon):
    X, y = [], []
    for i in range(len(data) - seq_len - horizon):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len:i+seq_len+horizon, 0])  # forecast feature 0
    return np.array(X), np.array(y)

X, y = create_sequences(data, SEQ_LEN, HORIZON)

# ===============================
# Walk-forward split
# ===============================
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

train_loader = DataLoader(
    TensorDataset(
        torch.tensor(X_train).float(),
        torch.tensor(y_train).float()
    ),
    batch_size=BATCH_SIZE,
    shuffle=True
)

# ===============================
# LSTM Model
# ===============================
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

# ===============================
# Hyperparameter tuning
# ===============================
results = []

for hidden_dim in [32, 64]:
    print(f"\nTraining model with hidden_dim = {hidden_dim}")

    model = LSTMModel(input_dim=5, hidden_dim=hidden_dim).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    loss_fn = nn.MSELoss()

    # Training
    model.train()
    for epoch in range(EPOCHS):
        epoch_loss = 0.0
        for xb, yb in train_loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            optimizer.zero_grad()
            preds = model(xb)
            loss = loss_fn(preds, yb)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        if (epoch + 1) % 5 == 0:
            print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {epoch_loss:.4f}")

    # ===============================
    # Evaluation (Walk-forward style)
    # ===============================
    model.eval()
    preds = []

    with torch.no_grad():
        for i in range(len(X_test)):
            x = torch.tensor(X_test[i:i+1]).float().to(DEVICE)
            pred = model(x)
            preds.append(pred.cpu().numpy()[0, 0])

    preds = np.array(preds)

    # ---- FIXED METRICS (version-safe) ----
    mse = mean_squared_error(y_test[:, 0], preds)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test[:, 0], preds)

    results.append((hidden_dim, rmse, mae))

# ===============================
# Final results
# ===============================
print("\n===============================")
print("Hyperparameter Tuning Results")
print("===============================")

for r in results:
    print(f"Hidden Units: {r[0]} | RMSE: {r[1]:.4f} | MAE: {r[2]:.4f}")

best_model = min(results, key=lambda x: x[1])
print("\nBest Configuration (Lowest RMSE):")
print(f"Hidden Units: {best_model[0]}, RMSE: {best_model[1]:.4f}, MAE: {best_model[2]:.4f}")